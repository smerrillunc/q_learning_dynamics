{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b6e7ee-6561-4457-9670-1189a8c2e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evoenv.envs import MatrixGame\n",
    "from evoenv.matrixtools import FloatTupleDtype\n",
    "import random\n",
    "import tqdm.notebook as tq\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7deaa3-efb7-402e-9bdc-2b5c344da4f2",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b873bfa7-9407-40b9-b0f9-95d6dbc02b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model = DQN(input_size, output_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.output_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(torch.tensor(state).float())\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        state_tensor = torch.tensor(state).float()\n",
    "        next_state_tensor = torch.tensor(next_state).float()\n",
    "        q_values = self.model(state_tensor)[action]\n",
    "        next_q_values = self.model(next_state_tensor).max().item()\n",
    "        target = reward + (1 - done) * self.gamma * next_q_values\n",
    "        loss = F.mse_loss(q_values, torch.tensor(target).float())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abceb35-0da4-46f4-b471-a3138797639e",
   "metadata": {},
   "source": [
    "### Running game utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80ad34f-aba2-4725-954c-577930e9603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_no_memory(agent1, agent2, env, num_episodes=100):\n",
    "    input_size = 1\n",
    "    output_size = 2\n",
    "        \n",
    "    state = 0\n",
    "    _, _ = env.reset()\n",
    "    \n",
    "    for episode in range(num_episodes):   \n",
    "        action1 = agent1.select_action([state])\n",
    "        action2 = agent2.select_action([state])\n",
    "                \n",
    "        next_state, actions, rewards, done = env.step((action1, action2))\n",
    "        \n",
    "        agent1.train([state], action1, rewards[0], [state], False)\n",
    "        agent2.train([state], action2, rewards[1], [state], False)\n",
    "    \n",
    "    return agent1, agent2\n",
    "\n",
    "def run_experiment(n_agents, env, num_episodes=100, num_games=100, shuffle=True):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    n_agents: in populaiton\n",
    "    env: environment to run\n",
    "    num_episodes: number of episodes in each matrix game to perform\n",
    "    num_games: number of rounds of pairings to perform\n",
    "    shuffle: same opponenet every game (False) or different opponent each game (True)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_size = 1\n",
    "    output_size = 2\n",
    "    \n",
    "    # initialize agents\n",
    "    agents = [DQNAgent(input_size=input_size, output_size=output_size) for x in range(n_agents)]\n",
    "    \n",
    "    for epoch in tq.tqdm(range(num_games)):\n",
    "        if shuffle:\n",
    "            random.shuffle(agents)\n",
    "        pairs = [(agents[i], agents[i+1]) for i in range(0, n_agents, 2)]\n",
    "    \n",
    "        # train each agent with their pair\n",
    "        for agent1, agent2 in pairs:\n",
    "            play_game_no_memory(agent1, agent2, env, num_episodes)\n",
    "\n",
    "    return agents\n",
    "\n",
    "def get_q_vals(agents):\n",
    "    all_q_vals = []\n",
    "    state_tensor = torch.tensor([0]).float()\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent_q = agent.model(state_tensor).detach().numpy()\n",
    "        all_q_vals.append(agent_q)\n",
    "        \n",
    "    return np.array(all_q_vals)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1fc951-773b-4322-b147-3fcc1ab46ad5",
   "metadata": {},
   "source": [
    "### No Memory Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2f355-48fb-4694-8f31-9746b06398e8",
   "metadata": {},
   "source": [
    "#### Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5547b657-8376-4c13-a60e-805e1ae315e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHICKEN GAME\n",
    "chickens_game_rewards = np.array([[(1, 1), (-1, 2)],\n",
    "                                 [(2,-1), (-2, -2)]], dtype=FloatTupleDtype(2))\n",
    "\n",
    "chickens_game_env = MatrixGame(chickens_game_rewards)\n",
    "_, _, = chickens_game_env.reset()\n",
    "\n",
    "# BATTLE OF SEXES\n",
    "battle_of_sexes_rewards = np.array([[(-1, -1), (1, 2)],\n",
    "                                 [(2,1), (-2, -2)]], dtype=FloatTupleDtype(2))\n",
    "\n",
    "battle_of_sexes_env = MatrixGame(battle_of_sexes_rewards)\n",
    "_, _, = battle_of_sexes_env.reset()\n",
    "\n",
    "\n",
    "# LET GEORGE DO IT\n",
    "LGDI_rewards = np.array([[(-1, -1), (2, 1)],\n",
    "                         [(1, 2), (-2, -2)]], dtype=FloatTupleDtype(2))\n",
    "\n",
    "LGDI_env = MatrixGame(LGDI_rewards)\n",
    "_, _, = LGDI_env.reset()\n",
    "\n",
    "# Prisoner's Dilemma\n",
    "PD_rewards = np.array([[(-1, -1), (2, -2)],\n",
    "                         [(-2,2), (1, 1)]], dtype=FloatTupleDtype(2))\n",
    "\n",
    "PD_env = MatrixGame(PD_rewards)\n",
    "_, _, = PD_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fb5a1-4375-43cb-957c-576d4f4fc80c",
   "metadata": {},
   "source": [
    "### Chicken Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f35ed-cf04-4a5f-bbb7-f8c996b6aa04",
   "metadata": {},
   "source": [
    "#### No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125df2b-297e-47b6-b965-b4924975f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65711527f1b74dc5b0998ac944f12145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents = run_experiment(n_agents=50, env=chickens_game_env, num_episodes=10000, num_games=1, shuffle=False)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Chicken Game No Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d974b-784e-48c2-815f-afe8142c0739",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f05ae-6377-4bbc-aac0-7e5681ca57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=chickens_game_env, num_episodes=100, num_games=100, shuffle=True)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Chicken Game Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39150c8d-abba-4278-9dc8-677569eec6d9",
   "metadata": {},
   "source": [
    "### Battle of the Sexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7f6ba-e87a-4f63-a9f7-1623e8085cd9",
   "metadata": {},
   "source": [
    "#### No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d7cac-4212-43b3-843a-b3184d5171f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=battle_of_sexes_env, num_episodes=10000, num_games=1, shuffle=False)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Battle of the Sexes No Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844b795-5cac-4644-a7a2-85d2d3322167",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e914c02-c5cd-4539-bb88-cba498f673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=battle_of_sexes_env, num_episodes=100, num_games=100, shuffle=True)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Battle of the Sexes Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4091ab-cd71-4403-8c6e-78a548b0e8a0",
   "metadata": {},
   "source": [
    "### Let George do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05463e-5f53-4f51-bd2f-323828038cc0",
   "metadata": {},
   "source": [
    "#### No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc089918-4f74-49de-a514-bab9b5a03eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=LGDI_env, num_episodes=10000, num_games=1, shuffle=False)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Let George do it No Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aeae44-b2f0-4814-afcf-a1ebb4c63e2c",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5b0e7-154d-490f-9100-0c7545592446",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=LGDI_env, num_episodes=100, num_games=100, shuffle=True)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Let George do it Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80a9a9-661b-4e88-80f6-353fff4bc021",
   "metadata": {},
   "source": [
    "### Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad282f54-0e90-42b4-8991-03adc19dbe62",
   "metadata": {},
   "source": [
    "#### No shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189ff5d-349e-4343-9d50-c1e128a24cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=PD_env, num_episodes=10000, num_games=1, shuffle=False)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Prisoner's Dilemma No Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a3694-f6fe-400b-96d2-3abc77d6e2f4",
   "metadata": {},
   "source": [
    "#### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0de03-2e23-4b2d-bfc4-15746596e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = run_experiment(n_agents=50, env=PD_env, num_episodes=100, num_games=100, shuffle=True)\n",
    "\n",
    "all_q_vals = get_q_vals(agents)\n",
    "plt.scatter(all_q_vals[:,0], all_q_vals[:,1])\n",
    "plt.xlabel(\"Action 0 Q Value\")\n",
    "plt.ylabel(\"Action 1 Q Value\")\n",
    "plt.title(\"Prisoner's Dilemma Shuffle Q-Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb608a-0a0f-4796-9cb3-8cdbd080eae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e38593-6ca7-4438-a35c-2be06aaa5c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coingame",
   "language": "python",
   "name": "coingame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
